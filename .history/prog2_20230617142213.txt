lb r0 00101
mov r15 r0
lbr r2
addi r0 00000
lbr r3
mov r0 r2
mov r5 r0
mov r0 r3
mov r6 r0
lb r0 01000
mov r4 r0
mov r14 r0
lb r1 00110
mov r0 r3
and r1
lb r0 01000
beq 1001
mov r0 r4
mov r1 r14
xor r4
mov r0 r2
lsl r2
mov r0 r3
lsl r3
lb r1 00111
mov r0 r14
and r12
addi r0 00000
mov r14 r0
blt 0001
mov r0 r4
lb r1 01000
beq 0010                    // if xor = 0, skip straight to shifting
mov r0 r5                   // r0 = lsb
rxor r0
mov r0 r6
rxor r0                     // r0 = ^{msb,lsb}
lb r1 01000
bne 0011                 // if parity != 0, there is exactly 1 error
lb r1 00110                 // r1 = 10000000
mov r6 r1
lb r0 01000
lb r1 01000
beq 0100                     // branch to end
lb r0 00110                 // correction bitmask
mov r7 r0                   // r7 = 10000000
mov r1 r4                   // r1 = r4
lb r0 01110
ble lsbcorrect              // if r4 < 8 then r8 (shift amt) = r4
mov r8 r1
lb r0 01000
lb r1 01000
beq next
mov r0 r1                   // r0 = r1 = r4
lb r1 01110
sub r8                      // r8 = r4 - 8
mov r0 r8                   // next
lb r1 01000                 // if r8 = 0, no need to shift
beq correct
mov r12 r0                  // shift loop
mov r0 r7                   // r0 = bitmask
and r11
lsr r7
mov r0 r12
addi r0 00000               // r12++
mov r12 r0
mov r1 r8
blt beginshiftloop
mov r1 r4
lb r0 01110
ble lsbxor                 // if r4 < 8 then correct the msb
mov r0 r7
mov r1 r6
xor r6
lb r0 01000
lb r1 01000
beq shift
mov r0 r7                  // lsbxor
mov r1 r5
xor r5
mov r0 r6                  // shifting
lsr r6
lb r1 01010
mov r0 r5
and r0
lsl r2
lb r1 00001
mov r0 r5
and r1
mov r0 r2
xor r5
lb r1 01100
mov r0 r5
xor r0
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
mov r0 r15                    // end
and r12
subi r0 00001
sbr r5
addi r0 00000
sbr r6
mov r0 r15
addi r0 00010
mov r15 r0
lb r1 01111
blt 1000
