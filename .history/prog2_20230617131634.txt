lb r0 00101
mov r15 r0
lbr r2
addi r0 00000
lbr r3
mov r0 r2
mov r5 r0
mov r0 r3
mov r6 r0
lb r0 01000
mov r4 r0
mov r14 r0
lb r1 00110
mov r0 r3
and r1
lb r0 01000
beq 1001
mov r0 r4
mov r1 r14
xor r4
mov r0 r2                    // b 1001
lsl r2
mov r0 r3
lsl r3
lb r1 00111
mov r0 r14
and r12
addi r0 00000
mov r14 r0
blt 0001
mov r0 r4
lb r1 01000
beq shift
mov r0 r5
lb r1 01001
and r0
rxor r0
mov r0 r6
rxor r0
lb r1 01000
bne correct
lb r1 00110
mov r6 r1
lb r0 01000
lb r1 01000
beq end
lb r0 00110                 // correction bitmask
mov r7 r0
mov r1 r4
lb r0 01110
ble lsbcorrect
mov r8 r1
lb r0 01000
lb r1 01000
beq next
mov r0 r1
lb r1 01110
sub r8
mov r0 r8                   // next
lb r1 01000
beq correct
mov r12 r0                  // shift loop
mov r0 r7
and r11
lsr r7
mov r0 r12
addi r0 00000
mov r12 r0
mov r1 r8
blt beginshiftloop
mov r1 r4
lb r0 01110
ble lsbxor
mov r0 r7
mov r1 r6
xor r6
lb r0 01000
lb r1 01000
beq shift
mov r0 r7                  // lsbxor
mov r1 r5                  // shifting
xor r5
mov r0 r6
lsr r6
lb r1 01010
mov r0 r5
and r0
lsl r2
lb r1 00001
mov r0 r5
and r1
mov r0 r2
xor r5
lb r1 01100
mov r0 r5
xor r0
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
and r12
mov r0 r6
lsr r6
mov r0 r5
lsr r5
